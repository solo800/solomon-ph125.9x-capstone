#===============================================================================
# CYO Project: Finding Home in France
# HarvardX PH125.9x Data Science Capstone
# Author: Adam Solomon
#===============================================================================

#-------------------------------------------------------------------------------
# 0. SETUP & CONFIGURATION
#-------------------------------------------------------------------------------

# Load required libraries
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(readxl)) install.packages("readxl", repos = "http://cran.us.r-project.org")
if(!require(scales)) install.packages("scales", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(here)) install.packages("here", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")
if(!require(xgboost)) install.packages("xgboost", repos = "http://cran.us.r-project.org")
if(!require(cluster)) install.packages("cluster", repos = "http://cran.us.r-project.org")

# Set paths
# Run this from the project root directory
PROJECT_ROOT <- here::here()
DATA_RAW <- file.path(PROJECT_ROOT, "data", "raw")
DATA_PROCESSED <- file.path(PROJECT_ROOT, "data")
LOCAL_DATA <- file.path(PROJECT_ROOT, "local_data")

# Display settings
options(scipen = 999)  # Disable scientific notation

#-------------------------------------------------------------------------------
# DATA ACCESS NOTE
#-------------------------------------------------------------------------------
# This script has two modes:
#
# QUICK-START (default for graders):
#   The processed dataset data/dvf_houses_clean.csv is included in the repo.
#   When this file exists, Sections 3–3B are skipped automatically and the
#   script proceeds directly to EDA and modeling. No raw DVF files needed.
#
# FULL PROCESSING (optional, for reproducibility):
#   Requires multi-GB DVF transaction files in local_data/ (not in repo).
#   Downloads from https://www.data.gouv.fr/fr/datasets/demandes-de-valeurs-foncieres/
#   Also requires data/raw/ source files (climate, population, election, Filosofi).
#
# Files that MUST be in the repo for the script to run:
#   - data/dvf_houses_clean.csv          (processed dataset, ~9 MB)
#   - data/commune_filter_lookup.csv     (159 target communes)
#   - data/raw/communes_2025.csv         (commune reference data)
#   - data/raw/population_age_brackets.xlsx
#   - data/raw/sunshine_climate_france.csv
#   - data/raw/filosofi_2020/FILO2020_DISP_DEP.csv
#   - data/raw/filosofi_2020/FILO2020_DISP_PAUVRES_DEP.csv
#   - data/raw/filosofi_2020_commune/base-cc-filosofi-2020-geo2023.xlsx
#   - data/raw/presidentielle_2022_tour1_departements.xlsx
#
# NOTE: commune_filter_lookup.csv was generated by scripts/commune_adjacency.R
# using IGN commune boundary polygons (sf::st_touches) to identify communes
# adjacent to each target city. The lookup file is included in the repository
# so this preprocessing step does not need to be repeated.
#-------------------------------------------------------------------------------

#-------------------------------------------------------------------------------
# 1. DATA LOADING
#-------------------------------------------------------------------------------

## 1.1 Load Communes Reference Data ----
communes <- read_csv(
file.path(DATA_RAW, "communes_2025.csv"),
  show_col_types = FALSE
)

## 1.2 Load Population by Age Data ----
# Sheet "COM" contains commune-level data
pop_age <- read_excel(
  file.path(DATA_RAW, "population_age_brackets.xlsx"),
  sheet = "COM"
)

## 1.3 Load Climate Data ----
climate <- read_csv(
  file.path(DATA_RAW, "sunshine_climate_france.csv"),
  show_col_types = FALSE
)

## 1.4 DVF Real Estate Data Loader ----
# DVF files are pipe-delimited, stored in local_data (not in repo).
# Only used when processing from scratch (Section 3.2).

# Function to load a single DVF file
load_dvf_file <- function(filepath) {
  read_delim(
    filepath,
    delim = "|",
    locale = locale(decimal_mark = ","),
    col_types = cols(
      `Code departement` = col_character(),
      `Code postal` = col_character(),
      `Code commune` = col_character()
    ),
    show_col_types = FALSE
  )
}

## 1.5 Load Economic Data (Filosofi 2020) ----

# Department-level income data
income_dept <- read_delim(
  file.path(DATA_RAW, "filosofi_2020", "FILO2020_DISP_DEP.csv"),
  delim = ";",
  show_col_types = FALSE
)

# Department-level poverty data
poverty_dept <- read_delim(
  file.path(DATA_RAW, "filosofi_2020", "FILO2020_DISP_PAUVRES_DEP.csv"),
  delim = ";",
  show_col_types = FALSE
)

#-------------------------------------------------------------------------------
# SECTION 1.6: LOAD PRESIDENTIAL ELECTION DATA
#-------------------------------------------------------------------------------
# Source: Ministère de l'Intérieur - data.gouv.fr
# Dataset: "Election présidentielle des 10 et 24 avril 2022 - Résultats définitifs du 1er tour"
# URL: https://www.data.gouv.fr/datasets/election-presidentielle-des-10-et-24-avril-2022-resultats-definitifs-du-1er-tour
# License: Open Licence 2.0

## 1.6.1 Download election results by department ----
election_url <- "https://www.data.gouv.fr/api/1/datasets/r/18847484-f622-4ccc-baa9-e6b12f749514"
election_file <- file.path(DATA_RAW, "presidentielle_2022_tour1_departements.xlsx")

# File is included in the repo; download is a fallback if missing
if (!file.exists(election_file)) {
  download.file(election_url, election_file, mode = "wb")
  message("Downloaded: Presidential election 2022 - 1st round by department")
}

## 1.6.2 Load and process election data ----
# The XLSX contains vote counts by candidate for each department
election_raw <- read_excel(election_file)

# Inspect column names (they may be in French with special characters)
# Expected columns include: Code du département, Libellé du département,
# and then pairs of columns for each candidate: Voix, % Voix/Exp

# The XLSX has repeating column groups per candidate. read_excel assigns
# positional names (...N) to the unnamed "% Voix/Exp" columns.
# Mapping (verified against the raw header row):
#   ...35 = Macron     (% Voix/Exp)
#   ...47 = Le Pen     (% Voix/Exp)
#   ...53 = Zemmour    (% Voix/Exp)
#   ...59 = Mélenchon  (% Voix/Exp)
election_dept <- election_raw |>
  transmute(
    code_departement = as.character(`Code du département`),
    dept_name_election = `Libellé du département`,
    pct_le_pen = as.numeric(`...47`),
    pct_macron = as.numeric(`...35`),
    pct_melenchon = as.numeric(`...59`),
    pct_zemmour = as.numeric(`...53`)
  ) |>
  mutate(code_departement = str_remove(code_departement, "^0"))

#-------------------------------------------------------------------------------
# 2. CITY SCREENING & SELECTION
#-------------------------------------------------------------------------------

## 2.1 Define Target Departments ----
# Final selection based on composite scoring (Section 2.9) combining climate,
# demographics, economics, and political alignment criteria.
# Paris (75) included as international mega-city benchmark.
TARGET_DEPTS <- c("13", "31", "33", "34", "69", "74", "75")
TARGET_DEPT_NAMES <- c(
  "13" = "Bouches-du-Rhône (Marseille)",
  "31" = "Haute-Garonne (Toulouse)",
  "33" = "Gironde (Bordeaux)",
  "34" = "Hérault (Montpellier)",
  "69" = "Rhône (Lyon)",
  "74" = "Haute-Savoie (Annecy)",
  "75" = "Paris (benchmark)"
)

## 2.2 Climate Screening ----
# Filter cities meeting sunshine threshold
SUNSHINE_THRESHOLD <- 2000  # hours per year

climate_qualified <- climate |>
  filter(sunshine_hours_annual >= SUNSHINE_THRESHOLD) |>
  arrange(desc(sunshine_hours_annual))

## 2.3 Department/Region Lookup ----
# Get unique department-to-region mapping for joining
dept_region_lookup <- communes |>
  select(code_departement, nom_departement, nom_region) |>
  distinct()

## 2.4 Aggregate Population by Age to Department Level ----

# Calculate population totals and % aged 25-54 by department
pop_by_dept <- pop_age |>
  # Remove the "D" prefix from DEP to match other datasets
  mutate(code_departement = str_remove(DEP, "^D")) |>
  # Group by department and sum all age brackets
  group_by(code_departement) |>
  summarise(
    pop_total = sum(`F0-2` + `F3-5` + `F6-10` + `F11-17` + `F18-24` + 
                      `F25-39` + `F40-54` + `F55-64` + `F65-79` + `F80+` +
                      `H0-2` + `H3-5` + `H6-10` + `H11-17` + `H18-24` + 
                      `H25-39` + `H40-54` + `H55-64` + `H65-79` + `H80+`, na.rm = TRUE),
    pop_25_54 = sum(`F25-39` + `F40-54` + `H25-39` + `H40-54`, na.rm = TRUE),
    .groups = "drop"
  ) |>
  mutate(pct_age_25_54 = pop_25_54 / pop_total * 100)

## 2.5 Create City Screening Dataset ----

# Join climate with department names and population data
city_screening <- climate |>
  # Remove leading zeros from department_code for matching
  mutate(code_departement = str_remove(department_code, "^0")) |>
  # Add department and region names
  left_join(dept_region_lookup, by = "code_departement") |>
  # Add population demographics
  left_join(pop_by_dept, by = "code_departement") |>
  # Clean up columns
  select(
    city_name,
    department_code,
    department_name = nom_departement,
    region_name = nom_region,
    pop_total,
    pct_age_25_54,
    sunshine_hours_annual,
    avg_temp_jan,
    avg_temp_jul,
    rainfall_mm_annual
  ) |>
  arrange(desc(sunshine_hours_annual))

## 2.6 Normalize Screening Variables ----

city_screening <- city_screening |>
  mutate(
    # Normalize sunshine (higher = better) -> 0 to 1
    sunshine_norm = (sunshine_hours_annual - min(sunshine_hours_annual, na.rm = TRUE)) /
      (max(sunshine_hours_annual, na.rm = TRUE) - min(sunshine_hours_annual, na.rm = TRUE)),
    
    # Normalize age demographic (higher % working age = better) -> 0 to 1
    age_norm = (pct_age_25_54 - min(pct_age_25_54, na.rm = TRUE)) /
      (max(pct_age_25_54, na.rm = TRUE) - min(pct_age_25_54, na.rm = TRUE)),
    
    # Normalize rainfall (lower = better, so invert) -> 0 to 1
    rainfall_norm = 1 - (rainfall_mm_annual - min(rainfall_mm_annual, na.rm = TRUE)) /
      (max(rainfall_mm_annual, na.rm = TRUE) - min(rainfall_mm_annual, na.rm = TRUE))
  )

# Verify the new columns
city_screening |>
  select(city_name, sunshine_hours_annual, sunshine_norm, 
         pct_age_25_54, age_norm, 
         rainfall_mm_annual, rainfall_norm) |>
  head(10)

## 2.7 Add Economic Indicators ----

# Extract and join economic metrics to city_screening
economic_metrics <- income_dept |>
  select(CODGEO, Q220, Q320) |>
  left_join(
    poverty_dept |> select(CODGEO, TP6020),
    by = "CODGEO"
  ) |>
  # Normalize department code to match city_screening (remove leading zeros)
  mutate(code_departement = str_remove(CODGEO, "^0")) |>
  select(-CODGEO)

# Join to city_screening and convert to numeric
city_screening <- city_screening |>
  mutate(code_departement = str_remove(department_code, "^0")) |>
  left_join(economic_metrics, by = "code_departement") |>
  mutate(
    median_income = as.numeric(Q220),
    affluent_income = as.numeric(Q320),
    poverty_rate = as.numeric(TP6020)
  ) |>
  select(-Q220, -Q320, -TP6020, -code_departement)

# Verify: cities with most affluent populations
city_screening |>
  select(city_name, department_name, median_income, affluent_income, poverty_rate) |>
  arrange(desc(affluent_income)) |>
  head(15)

# Normalize economic indicators
city_screening <- city_screening |>
  mutate(
    # Normalize affluent income (higher = better for wife's business)
    affluent_norm = (affluent_income - min(affluent_income, na.rm = TRUE)) /
      (max(affluent_income, na.rm = TRUE) - min(affluent_income, na.rm = TRUE)),
    
    # Normalize poverty rate (lower = better, so invert)
    poverty_norm = 1 - (poverty_rate - min(poverty_rate, na.rm = TRUE)) /
      (max(poverty_rate, na.rm = TRUE) - min(poverty_rate, na.rm = TRUE))
  )

# Verify target cities
city_screening |>
  filter(city_name %in% c("Marseille", "Toulouse", "Bordeaux", "Montpellier", 
                           "Lyon", "Annecy", "Paris")) |>
  select(city_name, affluent_income, affluent_norm, poverty_rate, poverty_norm)

#-------------------------------------------------------------------------------
# SECTION 2.8: ADD POLITICAL INDICATORS TO CITY SCREENING
#-------------------------------------------------------------------------------

## 2.8.1 Join election data to city_screening ----
city_screening <- city_screening |>
  left_join(
    election_dept |> select(code_departement, pct_le_pen, pct_zemmour),
    by = c("department_code" = "code_departement")
  ) |>
  # Calculate combined far-right vote (RN + Reconquête)
  mutate(
    pct_far_right = pct_le_pen + pct_zemmour
  )

## 2.8.2 Normalize political indicator ----
# Lower far-right vote = better (for this family's preferences)
# Inverted normalization: 0 = highest far-right, 1 = lowest far-right
city_screening <- city_screening |>
  mutate(
    far_right_norm = 1 - (pct_far_right - min(pct_far_right, na.rm = TRUE)) / 
      (max(pct_far_right, na.rm = TRUE) - min(pct_far_right, na.rm = TRUE))
  )

## 2.8.3 Verify the join for target cities ----
city_screening |>
  filter(department_code %in% TARGET_DEPTS) |>
  select(city_name, department_code, pct_le_pen, pct_zemmour, pct_far_right, far_right_norm) |>
  arrange(desc(pct_far_right))

#-------------------------------------------------------------------------------
# SECTION 2.9: COMPOSITE SCORING
#-------------------------------------------------------------------------------

## 2.9.1 Define weights ----
# Political alignment is primary criterion (family values)
# Climate factors (sunshine, rainfall) and affluence are secondary
# Age demographics tertiary, poverty minimal weight
# Weights iterated through multiple rounds — see project_status_region_selection.md
weights <- c(
  far_right = 1,
  sunshine  = 0.75,
  rainfall  = 0.75,
  affluent  = 0.75,
  age       = 0.5,
  poverty   = 0.25
)

## 2.9.2 Calculate weighted composite score ----
city_screening <- city_screening |>
  mutate(
    composite_score = (
      sunshine_norm  * weights["sunshine"] +
      rainfall_norm  * weights["rainfall"] +
      affluent_norm  * weights["affluent"] +
      far_right_norm * weights["far_right"] +
      age_norm       * weights["age"] +
      poverty_norm   * weights["poverty"]
    ) / sum(weights)
  )

## 2.9.3 Generate ranked city list ----
# Clean ranking: exclude Île-de-France and Corsica (personal constraints)
# Île-de-France: departing region; Corsica: island logistics impractical
city_screening |>
  filter(!region_name %in% c("Île-de-France", "Corse")) |>
  arrange(desc(composite_score)) |>
  mutate(rank = row_number()) |>
  select(rank, city_name, department_name, department_code, composite_score,
         sunshine_norm, rainfall_norm, affluent_norm, far_right_norm) |>
  print(n = 15)

## 2.9.4 Confirm target cities from composite ranking ----
# Final selection: Toulouse, Lyon, Annecy, Montpellier, Bordeaux + Paris benchmark
# Selection justified by composite score ranking and qualitative factors
# (TGV connections, international airports, university/tech presence)
city_screening |>
  filter(department_code %in% TARGET_DEPTS) |>
  arrange(desc(composite_score)) |>
  select(city_name, department_name, department_code, composite_score,
         sunshine_norm, rainfall_norm, affluent_norm, far_right_norm)

#-------------------------------------------------------------------------------
# 3. DVF DATA LOADING & FILTERING
#-------------------------------------------------------------------------------
# Load 2020-2024 DVF transaction files, filter to target communes (houses only),
# and join commune metadata (target_city, ring). Saves filtered dataset to CSV
# so subsequent runs can skip this step.

dvf_clean_path <- file.path(DATA_PROCESSED, "dvf_houses_clean.csv")

if (file.exists(dvf_clean_path)) {
  cat("Loading cached clean dataset...\n")
  dvf_houses <- read_csv(dvf_clean_path, show_col_types = FALSE,
                          col_types = cols(insee_code = col_character()))
  cat("Loaded:", nrow(dvf_houses), "rows from", dvf_clean_path, "\n")
} else {
  cat("Processing DVF files from scratch...\n\n")

## 3.1 Load Commune Filter Lookup ----
# 159 communes across 7 city groups (ring 0 = city proper, ring 1 = adjacent)
# Includes Paris suburb departments (92, 93, 94)
commune_lookup <- read_csv(
  file.path(DATA_PROCESSED, "commune_filter_lookup.csv"),
  col_types = cols(
    dept_code = col_character(),
    commune_code = col_character(),
    insee_code = col_character()
  )
)

target_dept_codes <- unique(commune_lookup$dept_code)
cat("Target departments:", paste(sort(target_dept_codes), collapse = ", "), "\n")
cat("Total communes to match:", nrow(commune_lookup), "\n")

## 3.2 Load and Filter DVF Files ----
# Process each year individually to manage memory (~500-600MB per file)
# Pre-filter by department, then match exact communes

dvf_files_to_load <- list.files(
  LOCAL_DATA,
  pattern = "ValeursFoncieres-(2020|2021|2022|2023|2024)",
  full.names = TRUE
)
cat("DVF files to load:\n")
cat(paste(" ", basename(dvf_files_to_load)), sep = "\n")

dvf_filtered_list <- map(dvf_files_to_load, function(f) {
  cat("\nProcessing:", basename(f), "...")

  raw <- load_dvf_file(f)
  cat(" loaded", format(nrow(raw), big.mark = ","), "rows ->")

  # Filter: target departments + houses + sales, then match communes
  filtered <- raw |>
    filter(
      `Code departement` %in% target_dept_codes,
      `Type local` == "Maison",
      `Nature mutation` == "Vente"
    ) |>
    mutate(
      insee_code = paste0(
        `Code departement`,
        str_pad(`Code commune`, 3, pad = "0")
      )
    ) |>
    filter(insee_code %in% commune_lookup$insee_code)

  cat(" filtered to", format(nrow(filtered), big.mark = ","), "house sales\n")
  filtered
})

## 3.3 Combine All Years ----
dvf_houses <- bind_rows(dvf_filtered_list)
cat("\n=== Total filtered house sales:", format(nrow(dvf_houses), big.mark = ","), "===\n")

# Free memory
rm(dvf_filtered_list)

## 3.4 Join Commune Metadata ----
dvf_houses <- dvf_houses |>
  left_join(
    commune_lookup |> select(insee_code, target_city, ring, commune_name),
    by = "insee_code"
  )

## 3.5 Parse Dates ----
dvf_houses <- dvf_houses |>
  mutate(
    date_mutation = dmy(`Date mutation`),
    year = year(date_mutation),
    month = month(date_mutation),
    quarter = quarter(date_mutation)
  )

## 3.6 Validation Summary ----
cat("\n--- Transactions by city ---\n")
dvf_houses |> count(target_city, sort = TRUE) |> print()

cat("\n--- Transactions by year ---\n")
dvf_houses |> count(year) |> print()

cat("\n--- Transactions by ring ---\n")
dvf_houses |> count(ring) |> print()

## 3.7 Save Filtered Dataset ----
write_csv(dvf_houses, file.path(DATA_PROCESSED, "dvf_houses_filtered.csv"))
cat("\nSaved:", file.path(DATA_PROCESSED, "dvf_houses_filtered.csv"), "\n")

#-------------------------------------------------------------------------------
# 3B. DATA CLEANING
#-------------------------------------------------------------------------------
# Raw filtered data: 68,006 rows
# Issues found in diagnostic:
#   - 5,302 multi-row mutation groups (12,175 rows) — same house on multiple parcels
#   - 64 NA prices, 7,786 NA land areas (11.4%)
#   - Outliers: symbolic €1 sales, €84M max, 1 m² built area, 54 rooms

n_start <- nrow(dvf_houses)

## 3B.1 Deduplicate Multi-Parcel Rows ----
# DVF repeats house rows when property spans multiple cadastral parcels.
# Same price/surface/rooms, different Section/No plan. Sum land area across parcels.
dvf_houses <- dvf_houses |>
  group_by(`No disposition`, `Date mutation`, `Valeur fonciere`,
           `Code departement`, `Code commune`) |>
  summarise(
    across(c(`Surface reelle bati`, `Nombre pieces principales`), max),
    `Surface terrain` = sum(`Surface terrain`, na.rm = TRUE),
    across(c(`Code postal`, Commune, insee_code, target_city, ring,
             commune_name, date_mutation, year, month, quarter,
             `No voie`, `Type de voie`, Voie), first),
    n_parcels = n(),
    .groups = "drop"
  )

cat("Dedup:", n_start, "->", nrow(dvf_houses),
    "(removed", n_start - nrow(dvf_houses), "duplicate parcel rows)\n")

## 3B.2 Drop Missing Prices ----
dvf_houses <- dvf_houses |>
  filter(!is.na(`Valeur fonciere`))

cat("After dropping NA prices:", nrow(dvf_houses), "\n")

## 3B.3 Outlier Filtering ----
# Conservative bounds for French houses in major metros
dvf_houses <- dvf_houses |>
  filter(
    `Valeur fonciere` >= 10000,          # Exclude symbolic/tax-free transfers
    `Valeur fonciere` <= 5000000,        # Exclude mega-estates
    `Surface reelle bati` >= 20,         # Min plausible house
    `Surface reelle bati` <= 1000,       # Max plausible house (not château)
    `Nombre pieces principales` >= 1,    # At least 1 room
    `Nombre pieces principales` <= 20    # Max plausible rooms
  )

cat("After outlier filtering:", nrow(dvf_houses), "\n")

## 3B.4 Feature Engineering ----
dvf_houses <- dvf_houses |>
  mutate(
    prix_m2 = `Valeur fonciere` / `Surface reelle bati`,
    has_land = !is.na(`Surface terrain`) & `Surface terrain` > 0
  )

# Remove extreme price/m² (catches remaining data quality issues)
q01 <- quantile(dvf_houses$prix_m2, 0.01, na.rm = TRUE)
q99 <- quantile(dvf_houses$prix_m2, 0.99, na.rm = TRUE)
cat("Price/m² 1st-99th percentile: ", round(q01), "-", round(q99), "EUR/m²\n")

dvf_houses <- dvf_houses |>
  filter(prix_m2 >= q01, prix_m2 <= q99)

cat("After prix_m2 trimming:", nrow(dvf_houses), "\n")

## 3B.5 Cleaning Summary ----
cat("\n=== CLEANING SUMMARY ===\n")
cat("Started:", n_start, "-> Final:", nrow(dvf_houses),
    "(", round((1 - nrow(dvf_houses)/n_start) * 100, 1), "% removed)\n")

cat("\n--- Final distribution by city ---\n")
dvf_houses |> count(target_city, sort = TRUE) |> print()

cat("\n--- Final distribution by year ---\n")
dvf_houses |> count(year) |> print()

cat("\n--- Price summary by city ---\n")
dvf_houses |>
  group_by(target_city) |>
  summarise(
    n = n(),
    median_price = median(`Valeur fonciere`),
    median_m2 = median(prix_m2),
    median_surface = median(`Surface reelle bati`),
    .groups = "drop"
  ) |>
  arrange(desc(median_m2)) |>
  print()

## 3B.6 Join Commune-Level Income Data ----
# Filosofi 2020 commune-level median income gives the model neighborhood-level
# economic context. Previously we only had department-level income (same value
# for all 159 communes within a department). This captures within-city variation.

filosofi_commune <- read_excel(
  file.path(DATA_RAW, "filosofi_2020_commune", "base-cc-filosofi-2020-geo2023.xlsx"),
  sheet = "COM",
  skip = 5
) |>
  select(
    code_commune_insee = CODGEO,
    median_income_commune = MED20,
    poverty_rate_commune = TP6020
  ) |>
  mutate(across(c(median_income_commune, poverty_rate_commune), as.numeric))

dvf_houses <- dvf_houses |>
  mutate(insee_code = as.character(insee_code)) |>
  left_join(filosofi_commune, by = c("insee_code" = "code_commune_insee"))

cat("Median income coverage:",
    sum(!is.na(dvf_houses$median_income_commune)), "/", nrow(dvf_houses),
    "transactions matched\n")

# Quick sanity check: income should vary within cities
dvf_houses |>
  group_by(target_city) |>
  summarise(
    min_income = min(median_income_commune, na.rm = TRUE),
    median_income = median(median_income_commune, na.rm = TRUE),
    max_income = max(median_income_commune, na.rm = TRUE),
    n_communes = n_distinct(insee_code),
    .groups = "drop"
  ) |>
  print()

## 3B.7 Save Complete Clean Dataset ----
write_csv(dvf_houses, dvf_clean_path)
cat("\nSaved:", dvf_clean_path, "\n")

} # end processing from scratch

#-------------------------------------------------------------------------------
# 3C. EXPLORATORY DATA ANALYSIS
#-------------------------------------------------------------------------------
# Examine distributions, city comparisons, temporal trends, and feature
# relationships before modeling. All plots use the clean dataset (59,373 rows).

# Consistent city color palette (ordered roughly south→north + benchmark)
city_colors <- c(
  "Marseille"   = "#E63946",  # warm red — Mediterranean
  "Montpellier" = "#F4A261",  # orange
  "Toulouse"    = "#E9C46A",  # gold
  "Bordeaux"    = "#2A9D8F",  # teal — Atlantic
  "Lyon"        = "#457B9D",  # steel blue
  "Annecy"      = "#7FB3D8",  # navy — Alpine
  "Paris"       = "#6C757D"   # grey — benchmark
)

# Reorder target_city as factor for consistent plot ordering (by median prix_m2)
city_order <- dvf_houses |>
  group_by(target_city) |>
  summarise(med = median(prix_m2), .groups = "drop") |>
  arrange(med) |>
  pull(target_city)

dvf_houses <- dvf_houses |>
  mutate(target_city = factor(target_city, levels = city_order))

## 3C.1 Overall Dataset Summary ----
cat("\n=== EXPLORATORY DATA ANALYSIS ===\n")
cat("Dataset:", nrow(dvf_houses), "house transactions across",
    n_distinct(dvf_houses$target_city), "cities,",
    min(dvf_houses$year), "-", max(dvf_houses$year), "\n\n")

## 3C.2 Price per m² by City ----
# Box plot — the key metric for cross-city comparison
ggplot(dvf_houses, aes(x = target_city, y = prix_m2, fill = target_city)) +
  geom_boxplot(outlier.alpha = 0.1, outlier.size = 0.5) +
  scale_y_continuous(labels = label_comma(big.mark = " ", suffix = " €/m²")) +
  scale_fill_manual(values = city_colors) +
  labs(
    title = "Price per m² by City",
    subtitle = "Box plot of €/m² — median, IQR, and outliers",
    x = NULL, y = "Price per m²"
  ) +
  theme_minimal(base_size = 12) +
  theme(legend.position = "none")

## 3C.3 Temporal Trends — Median Price/m² by Year ----
yearly_city <- dvf_houses |>
  group_by(target_city, year) |>
  summarise(
    n = n(),
    median_prix_m2 = median(prix_m2),
    median_price = median(`Valeur fonciere`),
    .groups = "drop"
  )

ggplot(yearly_city, aes(x = year, y = median_prix_m2,
                        color = target_city, group = target_city)) +
  geom_line(linewidth = 1) +
  geom_point(size = 2.5) +
  scale_y_continuous(labels = label_comma(big.mark = " ", suffix = " €/m²")) +
  scale_color_manual(values = city_colors) +
  labs(
    title = "Median Price per m² Over Time",
    subtitle = "Annual trend by city (2020–2024)",
    x = "Year", y = "Median €/m²", color = "City"
  ) +
  theme_minimal(base_size = 12) +
  theme(legend.position = "bottom")

# Print the underlying data
cat("\n--- Median price/m² by city and year ---\n")
yearly_city |>
  select(target_city, year, n, median_prix_m2) |>
  pivot_wider(names_from = year, values_from = c(n, median_prix_m2)) |>
  print()

## 3C.4 Transaction Volume Over Time ----
ggplot(yearly_city, aes(x = year, y = n, fill = target_city)) +
  geom_col(position = "dodge") +
  scale_fill_manual(values = city_colors) +
  scale_y_continuous(labels = label_comma()) +
  labs(
    title = "House Transaction Volume by City and Year",
    subtitle = "Note: 2020 includes only second semester",
    x = "Year", y = "Transactions", fill = "City"
  ) +
  theme_minimal(base_size = 12) +
  theme(legend.position = "bottom")

## 3C.5 Ring Effect — City Center vs Suburbs ----
dvf_houses <- dvf_houses |>
  mutate(ring_label = ifelse(ring == 0, "City proper", "Adjacent suburb"))

cat("\n--- Price/m² by ring and city ---\n")
dvf_houses |>
  group_by(target_city, ring_label) |>
  summarise(
    n = n(),
    median_prix_m2 = median(prix_m2),
    median_surface = median(`Surface reelle bati`),
    .groups = "drop"
  ) |>
  print(n = Inf)

ggplot(dvf_houses, aes(x = ring_label, y = prix_m2, fill = ring_label)) +
  geom_boxplot(outlier.alpha = 0.1, outlier.size = 0.5) +
  facet_wrap(~target_city, scales = "free_y", ncol = 4) +
  scale_y_continuous(labels = label_comma(big.mark = " ", suffix = " €/m²")) +
  scale_fill_brewer(palette = "Set2") +
  labs(
    title = "Price per m² — City Center vs Adjacent Suburbs",
    x = NULL, y = "Price per m²", fill = NULL
  ) +
  theme_minimal(base_size = 11) +
  theme(legend.position = "bottom",
        axis.text.x = element_blank())

## 3C.6 Income Effect — Same House, Different Neighborhood ----
# A ~100 m² house costs very different amounts depending on commune wealth.
# This slope chart splits each city's communes into low vs high income
# (median split) and shows the price gap for similarly-sized houses.
# Steep slopes = income explains price variance; flat = it doesn't.

slope_data <- dvf_houses |>
  filter(`Surface reelle bati` >= 85, `Surface reelle bati` <= 115) |>
  group_by(target_city) |>
  mutate(
    income_tier = ifelse(
      median_income_commune <= median(median_income_commune, na.rm = TRUE),
      "Low Income\nCommunes",
      "High Income\nCommunes"
    )
  ) |>
  ungroup() |>
  group_by(target_city, income_tier) |>
  summarise(
    median_price = median(`Valeur fonciere`, na.rm = TRUE),
    n = n(),
    .groups = "drop"
  )

# Order x-axis: low income on left, high income on right
slope_data <- slope_data |>
  mutate(income_tier = factor(income_tier,
                              levels = c("Low Income\nCommunes",
                                         "High Income\nCommunes")))

ggplot(slope_data, aes(x = income_tier, y = median_price,
                       group = target_city, color = target_city)) +
  geom_line(linewidth = 1.2) +
  geom_point(size = 3.5) +
  geom_text(
    data = slope_data |> filter(income_tier == "High Income\nCommunes"),
    aes(label = target_city),
    hjust = -0.15, size = 3.5
  ) +
  scale_y_continuous(labels = label_comma(prefix = "\u20ac")) +
  scale_color_manual(values = city_colors) +
  labs(
    title = "Same-Sized House, Different Price: The Income Effect",
    subtitle = "Median price of ~100 m\u00b2 houses in low vs high income communes",
    x = NULL, y = "Median House Price"
  ) +
  theme_minimal(base_size = 12) +
  theme(legend.position = "none")

## 3C.7 Feature Correlations ----
cat("\n--- Correlation matrix (numeric features) ---\n")
numeric_features <- dvf_houses |>
  select(`Valeur fonciere`, `Surface reelle bati`,
         `Nombre pieces principales`, `Surface terrain`, prix_m2, year) |>
  rename(
    price = `Valeur fonciere`,
    surface = `Surface reelle bati`,
    rooms = `Nombre pieces principales`,
    land = `Surface terrain`,
    price_m2 = prix_m2
  )

cor_matrix <- cor(numeric_features, use = "pairwise.complete.obs")
round(cor_matrix, 2) |> print()

cat("\nEDA complete. Proceeding to modeling.\n")

#-------------------------------------------------------------------------------
# 4. MODELING APPROACH
#-------------------------------------------------------------------------------

## 4.1 Train/Test Split ----
# Temporal split: train on 2020-2023, test on 2024.
# Real estate prices are path-dependent — random splitting would leak
# future market conditions into training data.

train <- dvf_houses |> filter(year <= 2023)
test  <- dvf_houses |> filter(year == 2024)

cat("\n=== TRAIN/TEST SPLIT ===\n")
cat("Train (2020-2023):", nrow(train), "rows (",
    round(nrow(train) / nrow(dvf_houses) * 100, 1), "%)\n")
cat("Test  (2024):     ", nrow(test), "rows (",
    round(nrow(test) / nrow(dvf_houses) * 100, 1), "%)\n")

cat("\n--- Train set by city ---\n")
train |> count(target_city, name = "n_train") |> print()

cat("\n--- Test set by city ---\n")
test |> count(target_city, name = "n_test") |> print()

cat("\n--- Train set by year ---\n")
train |> count(year) |> print()

## 4.2 Feature Preparation ----
# Select modeling features and target variable.
# Exclude prix_m2 (derived from target — would leak) and address/ID columns.
#
# SAMPLE SIZE IMBALANCE: Bordeaux has ~14K training rows vs Annecy ~1.8K.
# A pooled model's learned relationships (e.g., price ~ surface slope) will
# be dominated by high-volume cities. To address this:
#   - Linear regression: use target_city interaction terms so each city
#     learns its own coefficients (e.g., target_city * Surface reelle bati)
#   - Tree models (RF/XGBoost): handle city-specific patterns naturally
#     via splits — no special treatment needed
#   - All models: evaluate per-city RMSE/MAE alongside overall metrics
#     to confirm no city is systematically underserved

model_features <- c(
  "Surface reelle bati",       # built area (m²)
  "Nombre pieces principales", # number of rooms
  "Surface terrain",           # land area (m²)
  "target_city",               # city (factor, 7 levels)
  "ring",                      # 0 = city proper, 1 = adjacent suburb
  "year",                      # transaction year
  "quarter",                   # seasonality (1-4)
  "has_land",                  # whether property has land
  "median_income_commune"      # commune-level median income (Filosofi 2020)
)
target_var <- "Valeur fonciere"

# Ensure proper types for modeling
train <- train |>
  mutate(
    target_city = factor(target_city),
    ring = as.integer(ring),
    quarter = as.integer(quarter),
    has_land = as.integer(has_land)
  )

test <- test |>
  mutate(
    target_city = factor(target_city, levels = levels(train$target_city)),
    ring = as.integer(ring),
    quarter = as.integer(quarter),
    has_land = as.integer(has_land)
  )

# Build model matrices (target + features only)
# Rename to clean names — avoids backtick issues with randomForest/xgboost
clean_names <- c(
  price   = "Valeur fonciere",
  surface = "Surface reelle bati",
  rooms   = "Nombre pieces principales",
  land    = "Surface terrain",
  median_income = "median_income_commune"
)

train_model <- train |>
  select(all_of(c(target_var, model_features))) |>
  rename(!!!clean_names)
test_model <- test |>
  select(all_of(c(target_var, model_features))) |>
  rename(!!!clean_names)

cat("\n=== FEATURE PREPARATION ===\n")
cat("Target:", target_var, "\n")
cat("Features (", length(model_features), "):", paste(model_features, collapse = ", "), "\n")
cat("Train matrix:", nrow(train_model), "x", ncol(train_model), "\n")
cat("Test matrix: ", nrow(test_model), "x", ncol(test_model), "\n")

cat("\n--- Feature summary (train) ---\n")
str(train_model)

## 4.3 Evaluation Metrics ----
# Define once, use for all models. Collect results in a single tibble.

calc_rmse <- function(actual, predicted) {
  sqrt(mean((actual - predicted)^2, na.rm = TRUE))
}

calc_mae <- function(actual, predicted) {
  mean(abs(actual - predicted), na.rm = TRUE)
}

calc_r2 <- function(actual, predicted) {
  1 - sum((actual - predicted)^2) / sum((actual - mean(actual))^2)
}

# Evaluate a model's predictions: overall + per-city breakdown
evaluate_model <- function(actual, predicted, city, model_name) {
  overall <- tibble(
    model = model_name,
    scope = "Overall",
    n = length(actual),
    rmse = calc_rmse(actual, predicted),
    mae  = calc_mae(actual, predicted),
    r2   = calc_r2(actual, predicted)
  )
  per_city <- tibble(actual, predicted, city) |>
    group_by(city) |>
    summarise(
      n = n(),
      rmse = calc_rmse(actual, predicted),
      mae  = calc_mae(actual, predicted),
      r2   = calc_r2(actual, predicted),
      .groups = "drop"
    ) |>
    mutate(model = model_name) |>
    rename(scope = city) |>
    select(model, scope, n, rmse, mae, r2)
  bind_rows(overall, per_city)
}

# Master results table — each model appends to this
all_results <- tibble()

## 4.4 Linear Regression (Baseline) ----
# Interaction terms: target_city × Surface reelle bati gives each city
# its own price-per-m² slope. target_city × ring gives each city its own
# urban premium. Shared coefficients for rooms, land, year, quarter.

lm_formula <- price ~
  target_city * (surface + ring) +
  rooms +
  land +
  year +
  quarter +
  has_land +
  median_income

cat("\n=== LINEAR REGRESSION ===\n")
cat("Formula:", deparse(lm_formula, width.cutoff = 200), "\n\n")

lm_fit <- lm(lm_formula, data = train_model)

cat("--- Model summary ---\n")
cat("Coefficients:", length(coef(lm_fit)), "\n")
cat("R² (train):", round(summary(lm_fit)$r.squared, 4), "\n")
cat("Adj R² (train):", round(summary(lm_fit)$adj.r.squared, 4), "\n\n")

# Predict on test set
lm_pred_test <- predict(lm_fit, newdata = test_model)

# Evaluate
lm_results <- evaluate_model(
  actual     = test_model$price,
  predicted  = lm_pred_test,
  city       = as.character(test_model$target_city),
  model_name = "Linear Regression"
)
all_results <- bind_rows(all_results, lm_results)

cat("--- Test set performance ---\n")
lm_results |> print(n = Inf)

cat("\n--- Top 10 coefficients by magnitude ---\n")
coef_tbl <- tibble(
  term = names(coef(lm_fit)),
  estimate = coef(lm_fit)
) |>
  filter(!is.na(estimate)) |>
  arrange(desc(abs(estimate)))
coef_tbl |> head(10) |> print()

## 4.5 Random Forest ----
# RF handles factor variables and non-linear relationships natively.
# No interaction terms needed — tree splits discover them automatically.
# This is the key advantage over linear regression for city-specific patterns.

cat("\n=== RANDOM FOREST ===\n")

set.seed(42)
rf_fit <- randomForest(
  price ~ .,
  data = train_model,
  ntree = 500,
  importance = TRUE
)

cat("Trees:", rf_fit$ntree, "\n")
cat("Variables tried at each split:", rf_fit$mtry, "\n")
cat("R2 (OOB):", round(1 - rf_fit$mse[rf_fit$ntree] /
      var(train_model$price), 4), "\n\n")

# Predict on test set
rf_pred_test <- predict(rf_fit, newdata = test_model)

# Evaluate
rf_results <- evaluate_model(
  actual     = test_model$price,
  predicted  = rf_pred_test,
  city       = as.character(test_model$target_city),
  model_name = "Random Forest"
)
all_results <- bind_rows(all_results, rf_results)

cat("--- Test set performance ---\n")
rf_results |> print(n = Inf)

# Variable importance
cat("\n--- Variable importance (% increase in MSE when permuted) ---\n")
importance(rf_fit) |>
  as.data.frame() |>
  rownames_to_column("feature") |>
  arrange(desc(`%IncMSE`)) |>
  print()

## 4.6 XGBoost ----
# Gradient-boosted trees — requires numeric matrix input.
# One-hot encode target_city; XGBoost learns interactions via boosting.

cat("\n=== XGBOOST ===\n")

# --- Validation split for early stopping ---
# Hold out 20% of training data so early stopping does not leak test set info.
# Without this, the test set would influence model selection (number of rounds),
# producing optimistically biased test metrics.
set.seed(42)
val_idx <- sample(nrow(train_model), size = floor(0.2 * nrow(train_model)))
train_model_xgb <- train_model[-val_idx, ]
val_model_xgb   <- train_model[val_idx, ]

cat("XGBoost train/val split:",
    nrow(train_model_xgb), "train /", nrow(val_model_xgb), "val\n")

# Build numeric matrices (one-hot encode factors)
train_x <- model.matrix(
  ~ surface + rooms + land + target_city + ring +
    year + quarter + has_land + median_income,
  data = train_model_xgb
)[, -1]

val_x <- model.matrix(
  ~ surface + rooms + land + target_city + ring +
    year + quarter + has_land + median_income,
  data = val_model_xgb
)[, -1]

test_x <- model.matrix(
  ~ surface + rooms + land + target_city + ring +
    year + quarter + has_land + median_income,
  data = test_model
)[, -1]

train_y <- train_model_xgb$price
val_y   <- val_model_xgb$price
test_y  <- test_model$price

dtrain <- xgb.DMatrix(data = train_x, label = train_y)
dval   <- xgb.DMatrix(data = val_x, label = val_y)
dtest  <- xgb.DMatrix(data = test_x, label = test_y)

# Train with early stopping to prevent overfitting
set.seed(42)
xgb_fit <- xgb.train(
  params = list(
    objective = "reg:squarederror",
    max_depth = 6,
    eta = 0.1,
    subsample = 0.8,
    colsample_bytree = 0.8
  ),
  data = dtrain,
  nrounds = 1000,
  evals = list(train = dtrain, val = dval),
  early_stopping_rounds = 50,
  verbose = 0
)

# Extract best iteration from evaluation log
xgb_log <- attr(xgb_fit, "evaluation_log")
xgb_best_iter <- which.min(xgb_log$val_rmse)
xgb_best_rmse <- xgb_log$val_rmse[xgb_best_iter]
cat("Stopped at:", nrow(xgb_log), "rounds (best:", xgb_best_iter, ")\n")
cat("Best val RMSE:", round(xgb_best_rmse, 0), "\n\n")

# Predict on test set
xgb_pred_test <- predict(xgb_fit, dtest)

# Evaluate
xgb_results <- evaluate_model(
  actual     = test_y,
  predicted  = xgb_pred_test,
  city       = as.character(test_model$target_city),
  model_name = "XGBoost"
)
all_results <- bind_rows(all_results, xgb_results)

cat("--- Test set performance ---\n")
xgb_results |> print(n = Inf)

# Feature importance
cat("\n--- XGBoost feature importance (top 10 by gain) ---\n")
xgb.importance(model = xgb_fit) |> head(10) |> print()

## 4.7 K-Means Clustering ----
# Unsupervised approach: cluster communes by real estate market characteristics
# to reveal market tiers that span cities.

cat("\n=== K-MEANS CLUSTERING ===\n")

# --- Step 1: Aggregate transactions to commune level ---
commune_summary <- dvf_houses |>
  group_by(insee_code, commune_name, target_city, ring) |>
  summarise(
    median_prix_m2 = median(prix_m2, na.rm = TRUE),
    median_surface = median(`Surface reelle bati`, na.rm = TRUE),
    median_rooms   = median(`Nombre pieces principales`, na.rm = TRUE),
    median_land    = median(`Surface terrain`, na.rm = TRUE),
    n_transactions = n(),
    median_income  = median(median_income_commune, na.rm = TRUE),
    .groups = "drop"
  ) |>
  filter(n_transactions >= 10, !is.na(median_income))

cat("Communes with >= 10 transactions:", nrow(commune_summary), "\n")
cat("\n--- Communes per city ---\n")
commune_summary |> count(target_city, sort = TRUE) |> print()

# --- Step 2: Scale features ---
cluster_features <- c("median_prix_m2", "median_surface", "median_rooms",
                       "median_land", "n_transactions", "median_income")
commune_scaled <- scale(commune_summary[, cluster_features])

cat("\nClustering features (", length(cluster_features), "):",
    paste(cluster_features, collapse = ", "), "\n")

# --- Step 3: Determine optimal k ---
set.seed(42)
k_range <- 2:8

# Total within-cluster sum of squares for each k
wss <- sapply(k_range, function(k) {
  kmeans(commune_scaled, centers = k, nstart = 25)$tot.withinss
})

# Average silhouette width for each k
avg_sil <- sapply(k_range, function(k) {
  km <- kmeans(commune_scaled, centers = k, nstart = 25)
  ss <- cluster::silhouette(km$cluster, dist(commune_scaled))
  mean(ss[, "sil_width"])
})

elbow_data <- tibble(k = k_range, wss = wss, silhouette = avg_sil)

cat("\n--- Elbow & silhouette analysis ---\n")
elbow_data |> mutate(across(c(wss, silhouette), ~round(., 3))) |> print()

# Elbow plot
ggplot(elbow_data, aes(x = k, y = wss)) +
  geom_line(linewidth = 0.8) +
  geom_point(size = 3) +
  scale_x_continuous(breaks = k_range) +
  labs(title = "K-Means: Elbow Method",
       x = "Number of Clusters (k)", y = "Total Within-Cluster SS") +
  theme_minimal(base_size = 12)

# Silhouette plot
ggplot(elbow_data, aes(x = k, y = silhouette)) +
  geom_line(linewidth = 0.8) +
  geom_point(size = 3) +
  scale_x_continuous(breaks = k_range) +
  labs(title = "K-Means: Average Silhouette Width",
       x = "Number of Clusters (k)", y = "Avg. Silhouette Width") +
  theme_minimal(base_size = 12)

# Select best k (highest silhouette)
best_k <- k_range[which.max(avg_sil)]
cat("\nBest k by silhouette:", best_k, "(avg silhouette:",
    round(max(avg_sil), 3), ")\n")

# Override: k=6 maximises silhouette but produces singleton/tiny clusters
# (1-2 communes) that aren't interpretable. k=4 has nearly identical
# silhouette (0.404 vs 0.414) while producing actionable market tiers.
FINAL_K <- 4
cat("Selected k:", FINAL_K,
    "(silhouette-optimal k =", best_k, "overridden — see rationale above)\n")

# --- Step 4: Run final K-means ---
set.seed(42)
km_fit <- kmeans(commune_scaled, centers = FINAL_K, nstart = 25)
commune_summary$cluster <- factor(km_fit$cluster)

cat("\n--- Cluster sizes ---\n")
table(commune_summary$cluster) |> print()

# --- Step 5: Analyze & visualize ---

# 5a. PCA biplot
pca_fit <- prcomp(commune_scaled)
pca_var <- summary(pca_fit)$importance[2, 1:2]  # proportion of variance

pca_df <- tibble(
  PC1 = pca_fit$x[, 1],
  PC2 = pca_fit$x[, 2],
  cluster = commune_summary$cluster,
  target_city = commune_summary$target_city,
  commune_name = commune_summary$commune_name
)

ggplot(pca_df, aes(x = PC1, y = PC2, color = cluster, shape = target_city)) +
  geom_point(size = 2.5, alpha = 0.8) +
  scale_shape_manual(values = c(16, 17, 15, 3, 7, 8, 18)) +
  labs(title = "K-Means Clusters: PCA Biplot",
       x = paste0("PC1 (", round(pca_var[1] * 100, 1), "% variance)"),
       y = paste0("PC2 (", round(pca_var[2] * 100, 1), "% variance)"),
       color = "Cluster", shape = "City") +
  theme_minimal(base_size = 12) +
  theme(legend.position = "bottom",
        legend.box = "vertical")

# 5b. Cluster centroids (original units)
centroids <- commune_summary |>
  group_by(cluster) |>
  summarise(
    n_communes     = n(),
    median_prix_m2 = round(mean(median_prix_m2)),
    median_surface = round(mean(median_surface)),
    median_rooms   = round(mean(median_rooms), 1),
    median_land    = round(mean(median_land)),
    n_transactions = round(mean(n_transactions)),
    median_income  = round(mean(median_income)),
    .groups = "drop"
  ) |>
  arrange(desc(median_prix_m2))

cat("\n--- Cluster centroids (original units) ---\n")
centroids |> print()

# 5c. City × cluster distribution
city_cluster <- commune_summary |>
  count(target_city, cluster) |>
  group_by(target_city) |>
  mutate(pct = n / sum(n)) |>
  ungroup()

cat("\n--- City x cluster distribution ---\n")
city_cluster |> print(n = Inf)

ggplot(city_cluster, aes(x = target_city, y = n, fill = cluster)) +
  geom_col(position = "fill") +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(title = "Market Cluster Distribution by City",
       x = NULL, y = "Proportion of Communes", fill = "Cluster") +
  theme_minimal(base_size = 12) +
  theme(axis.text.x = element_text(angle = 30, hjust = 1),
        legend.position = "bottom")

cat("\nK-means complete (k =", FINAL_K, "):",
    nrow(commune_summary), "communes clustered.\n")

#-------------------------------------------------------------------------------
# 5. RESULTS
#-------------------------------------------------------------------------------

## 5.2 Model Performance Comparison ----

cat("\n=== MODEL PERFORMANCE COMPARISON ===\n")

### 5.2.1 Overall Model Comparison Table ----

cat("\n--- Overall model metrics (test set: 2024) ---\n")

overall <- all_results |>
  filter(scope == "Overall") |>
  mutate(
    rmse_fmt = paste0("€", scales::comma(round(rmse))),
    mae_fmt  = paste0("€", scales::comma(round(mae))),
    r2_fmt   = sprintf("%.3f", r2)
  )

overall |>
  select(model, n, rmse_fmt, mae_fmt, r2_fmt) |>
  print()

# Improvement of XGBoost over Linear Regression
lm_overall  <- overall |> filter(model == "Linear Regression")
xgb_overall <- overall |> filter(model == "XGBoost")

rmse_reduction_pct <- (lm_overall$rmse - xgb_overall$rmse) / lm_overall$rmse * 100
r2_gain <- xgb_overall$r2 - lm_overall$r2

cat("\n--- XGBoost vs Linear Regression ---\n")
cat("RMSE reduction:", round(rmse_reduction_pct, 1), "%\n")
cat("R² improvement:", sprintf("+%.3f", r2_gain),
    paste0("(", round(lm_overall$r2, 3), " → ", round(xgb_overall$r2, 3), ")"), "\n")

### 5.2.2 Overall Model Comparison — R² Bar Chart ----

# Horizontal bar chart of R² across the 3 models
overall_plot <- overall |>
  mutate(model = fct_reorder(model, r2))

ggplot(overall_plot, aes(x = model, y = r2, fill = model)) +
  geom_col(width = 0.6) +
  geom_text(aes(label = sprintf("%.3f", r2)), hjust = -0.15, size = 4) +
  coord_flip(ylim = c(0, max(overall$r2) * 1.12)) +
  scale_fill_manual(values = c(
    "Linear Regression" = "#457B9D",
    "Random Forest"     = "#2A9D8F",
    "XGBoost"           = "#E63946"
  )) +
  labs(
    title = "Overall R² by Model (Test Set: 2024)",
    x = NULL, y = "R²"
  ) +
  theme_minimal(base_size = 12) +
  theme(legend.position = "none")

### 5.2.3 Per-City R² Comparison ----

# Order cities by XGBoost R²
xgb_city_order <- all_results |>
  filter(model == "XGBoost", scope != "Overall") |>
  arrange(r2) |>
  pull(scope)

per_city <- all_results |>
  filter(scope != "Overall") |>
  mutate(scope = factor(scope, levels = xgb_city_order))

# Overall XGBoost R² for reference line
xgb_overall_r2 <- xgb_overall$r2

model_colors <- c(
  "Linear Regression" = "#457B9D",
  "Random Forest"     = "#2A9D8F",
  "XGBoost"           = "#E63946"
)

ggplot(per_city, aes(x = scope, y = r2, fill = model)) +
  geom_col(position = position_dodge(width = 0.8), width = 0.7) +
  geom_hline(yintercept = xgb_overall_r2, linetype = "dashed",
             color = "grey30", linewidth = 0.5) +
  annotate("text", x = 0.5, y = xgb_overall_r2 + 0.015,
           label = paste0("Overall R² = ", round(xgb_overall_r2, 3)),
           hjust = 0, size = 3.2, color = "grey30") +
  scale_fill_manual(values = model_colors) +
  labs(
    title = "R² by City and Model",
    subtitle = "Dashed line = XGBoost overall R²",
    x = NULL, y = "R²", fill = "Model"
  ) +
  theme_minimal(base_size = 12) +
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 30, hjust = 1))

### 5.2.4 Feature Importance Comparison (RF vs XGBoost) ----

cat("\n--- Feature importance comparison ---\n")

# RF importance: %IncMSE, normalized to proportions
rf_imp <- importance(rf_fit) |>
  as.data.frame() |>
  rownames_to_column("feature") |>
  select(feature, importance = `%IncMSE`) |>
  mutate(importance = importance / sum(importance),
         model = "Random Forest")

# XGBoost importance: Gain column (already proportions)
xgb_imp_raw <- xgb.importance(model = xgb_fit) |>
  select(feature = Feature, importance = Gain)

# Group one-hot encoded features back to original names
# target_cityParis, target_cityLyon, etc. → "target_city"
xgb_imp <- xgb_imp_raw |>
  mutate(
    feature = case_when(
      str_detect(feature, "^target_city") ~ "target_city",
      TRUE ~ feature
    )
  ) |>
  group_by(feature) |>
  summarise(importance = sum(importance), .groups = "drop") |>
  mutate(model = "XGBoost")

# Combine and create unified feature set
fi_combined <- bind_rows(rf_imp, xgb_imp)

# Rank by average importance across both models
feature_order <- fi_combined |>
  group_by(feature) |>
  summarise(avg = mean(importance), .groups = "drop") |>
  arrange(avg) |>
  pull(feature)

fi_combined <- fi_combined |>
  mutate(feature = factor(feature, levels = feature_order))

cat("Feature importance (combined):\n")
fi_combined |>
  mutate(importance = round(importance, 3)) |>
  pivot_wider(names_from = model, values_from = importance) |>
  arrange(desc(`XGBoost`)) |>
  print()

# Side-by-side horizontal bar chart
ggplot(fi_combined, aes(x = feature, y = importance, fill = model)) +
  geom_col(width = 0.7) +
  coord_flip() +
  facet_wrap(~model) +
  scale_fill_manual(values = c(
    "Random Forest" = "#2A9D8F",
    "XGBoost"       = "#E63946"
  )) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(
    title = "Feature Importance: Random Forest vs XGBoost",
    subtitle = "RF = %IncMSE (normalized) | XGBoost = Gain (proportional)",
    x = NULL, y = "Relative Importance"
  ) +
  theme_minimal(base_size = 12) +
  theme(legend.position = "none")

### 5.2.5 Residual Analysis (XGBoost) ----

cat("\n--- Residual analysis (XGBoost) ---\n")

# Calculate residuals
residual_df <- test_model |>
  mutate(
    predicted = xgb_pred_test,
    residual  = price - xgb_pred_test
  )

# Sample for readability
set.seed(42)
resid_sample <- residual_df |> slice_sample(n = min(3000, nrow(residual_df)))

ggplot(resid_sample, aes(x = predicted, y = residual, color = target_city)) +
  geom_point(alpha = 0.3, size = 1) +
  geom_hline(yintercept = 0, linewidth = 0.6, color = "black") +
  scale_x_continuous(labels = scales::comma_format(prefix = "€")) +
  scale_y_continuous(labels = scales::comma_format(prefix = "€")) +
  scale_color_manual(values = city_colors) +
  labs(
    title = "Residual Plot: XGBoost Predictions vs Errors",
    subtitle = paste0("Sample of ", nrow(resid_sample), " test-set transactions"),
    x = "Predicted Price", y = "Residual (Actual − Predicted)", color = "City"
  ) +
  theme_minimal(base_size = 12) +
  theme(legend.position = "bottom")

# Residuals show mild heteroscedasticity — typical for real estate pricing

### 5.2.6 Impact of Median Income Feature ----

cat("\n--- Impact of commune-level median income ---\n")

# "Before" values from a prior run of this pipeline WITHOUT the
# median_income_commune feature. Hardcoded because the current script
# always includes median_income. See report for full discussion.
before <- tibble(
  model = c("Linear Regression", "Random Forest", "XGBoost"),
  r2_before   = c(0.554, 0.597, 0.599),
  rmse_before = c(263169, 250315, 249607)
)

# "After" values from current results
after <- all_results |>
  filter(scope == "Overall") |>
  select(model, r2_after = r2, rmse_after = rmse)

income_impact <- before |>
  left_join(after, by = "model") |>
  mutate(
    r2_gain   = r2_after - r2_before,
    rmse_drop = rmse_before - rmse_after,
    rmse_drop_pct = rmse_drop / rmse_before * 100
  )

# Print summary
income_impact |>
  mutate(
    across(starts_with("r2"), ~sprintf("%.3f", .)),
    rmse_before = paste0("€", scales::comma(round(rmse_before))),
    rmse_after  = paste0("€", scales::comma(round(rmse_after))),
    rmse_drop   = paste0("€", scales::comma(round(rmse_drop))),
    rmse_drop_pct = paste0(round(as.numeric(rmse_drop_pct), 1), "%")
  ) |>
  print()

# Before/after R² grouped bar chart
income_plot_data <- income_impact |>
  select(model, Before = r2_before, After = r2_after) |>
  pivot_longer(cols = c(Before, After), names_to = "stage", values_to = "value") |>
  mutate(stage = factor(stage, levels = c("Before", "After")))

ggplot(income_plot_data, aes(x = model, y = value, fill = stage)) +
  geom_col(position = position_dodge(width = 0.7), width = 0.6) +
  geom_text(aes(label = sprintf("%.3f", value)),
            position = position_dodge(width = 0.7), vjust = -0.5, size = 3.5) +
  scale_fill_manual(values = c("Before" = "#ADB5BD", "After" = "#E63946")) +
  scale_y_continuous(limits = c(0, max(income_plot_data$value) * 1.1)) +
  labs(
    title = "Impact of Adding Commune-Level Median Income",
    subtitle = "R² before vs after adding median_income feature",
    x = NULL, y = "R²", fill = NULL
  ) +
  theme_minimal(base_size = 12) +
  theme(legend.position = "bottom")

## 5.3 City Rankings ----

cat("\n=== CITY RANKINGS ===\n")

# --- Ensure required objects are available ---
if (!exists("city_screening")) {
  cat("Warning: city_screening not found.",
      "Screening-based metrics will be NA.\n")
}
if (!exists("TARGET_DEPTS")) {
  TARGET_DEPTS <- c("13", "31", "33", "34", "69", "74", "75")
}
if (!exists("FINAL_K")) FINAL_K <- 4

### 5.3.1 City Real Estate Summary Table ----

cat("\n--- 5.3.1 City real estate summary ---\n")

# Aggregate DVF data by city
city_summary <- dvf_houses |>
  group_by(target_city) |>
  summarise(
    n_transactions = n(),
    median_price   = median(`Valeur fonciere`, na.rm = TRUE),
    median_eur_m2  = median(prix_m2, na.rm = TRUE),
    median_surface = median(
      `Surface reelle bati`, na.rm = TRUE
    ),
    median_rooms   = median(
      `Nombre pieces principales`, na.rm = TRUE
    ),
    .groups = "drop"
  )

# Join XGBoost per-city metrics
xgb_city <- all_results |>
  filter(model == "XGBoost", scope != "Overall") |>
  select(target_city = scope, xgb_r2 = r2, xgb_mae = mae)

city_summary <- city_summary |>
  left_join(xgb_city, by = "target_city")

# Join screening data (composite score, sunshine)
if (exists("city_screening")) {
  screening_join <- city_screening |>
    filter(department_code %in% TARGET_DEPTS) |>
    select(city_name, composite_score, sunshine_hours_annual)
  city_summary <- city_summary |>
    left_join(
      screening_join,
      by = c("target_city" = "city_name")
    )
}

# Join cluster diversity from commune_summary
if (exists("commune_summary") &&
    "cluster" %in% names(commune_summary)) {
  cluster_diversity <- commune_summary |>
    group_by(target_city) |>
    summarise(
      n_clusters = n_distinct(cluster),
      dominant_cluster = names(which.max(table(cluster))),
      .groups = "drop"
    )
  city_summary <- city_summary |>
    left_join(cluster_diversity, by = "target_city")
}

# Print comprehensive 7-city comparison
city_summary |>
  mutate(
    median_price  = paste0(
      "\u20ac", scales::comma(round(median_price))
    ),
    median_eur_m2 = paste0(
      "\u20ac", scales::comma(round(median_eur_m2))
    ),
    xgb_r2  = sprintf("%.3f", xgb_r2),
    xgb_mae = paste0(
      "\u20ac", scales::comma(round(xgb_mae))
    )
  ) |>
  print()

### 5.3.2 Standard House Price Prediction ----

cat("\n--- 5.3.2 Standard house price prediction ---\n")
cat("Standard house: 100 m\u00b2, 4 rooms, 500 m\u00b2 land,",
    "city proper, 2024 Q2\n\n")

# City-specific median incomes (from training-era data)
city_incomes <- dvf_houses |>
  filter(year <= 2023) |>
  group_by(target_city) |>
  summarise(
    median_income = median(
      median_income_commune, na.rm = TRUE
    ),
    .groups = "drop"
  )

# Get factor levels from test_model
city_levels <- levels(test_model$target_city)

# Build prediction data frame — one row per city
std_house <- tibble(
  target_city = factor(city_levels, levels = city_levels),
  surface     = 100,
  rooms       = 4L,
  land        = 500,
  ring        = 0L,
  year        = 2024L,
  quarter     = 2L,
  has_land    = 1L
) |>
  left_join(city_incomes, by = "target_city")

# Build model matrix (same formula as Section 4.6)
std_x <- model.matrix(
  ~ surface + rooms + land + target_city + ring +
    year + quarter + has_land + median_income,
  data = std_house
)[, -1]

# Predict with XGBoost
std_dmat <- xgb.DMatrix(data = std_x)
std_house$predicted_price <- predict(xgb_fit, std_dmat)

# Print ranked by price (cheapest first)
std_house |>
  arrange(predicted_price) |>
  mutate(
    rank = row_number(),
    price_fmt = paste0(
      "\u20ac", scales::comma(round(predicted_price))
    ),
    income_fmt = paste0(
      "\u20ac", scales::comma(round(median_income))
    )
  ) |>
  select(rank, target_city, price_fmt, income_fmt) |>
  print()

### 5.3.3 Affordability Lollipop Chart ----

cat("\n--- 5.3.3 Affordability lollipop chart ---\n")

lollipop_data <- std_house |>
  mutate(
    city_label = fct_reorder(
      target_city, predicted_price
    ),
    price_label = paste0(
      "\u20ac",
      scales::comma(round(predicted_price / 1000)),
      "k"
    )
  )

ggplot(lollipop_data,
       aes(x = city_label, y = predicted_price)) +
  geom_segment(
    aes(xend = city_label, y = 0, yend = predicted_price,
        color = target_city),
    linewidth = 1.2
  ) +
  geom_point(aes(color = target_city), size = 4) +
  geom_text(aes(label = price_label),
            hjust = -0.3, size = 3.5) +
  scale_y_continuous(
    labels = scales::comma_format(prefix = "\u20ac"),
    expand = expansion(mult = c(0, 0.15))
  ) +
  scale_color_manual(values = city_colors) +
  coord_flip() +
  labs(
    title = "Predicted Price: Standard House by City",
    subtitle = paste(
      "100 m\u00b2, 4 rooms, 500 m\u00b2 land,",
      "city proper, 2024 Q2"
    ),
    x = NULL, y = "Predicted Price"
  ) +
  theme_minimal(base_size = 12) +
  theme(legend.position = "none")

### 5.3.4 Multi-Criteria City Comparison ----

cat("\n--- 5.3.4 Multi-criteria city comparison ---\n")

# Build normalized dimensions (all 0-1, higher = better)
criteria <- std_house |>
  select(target_city, predicted_price) |>
  mutate(
    affordability = 1 - (
      predicted_price - min(predicted_price)
    ) / (max(predicted_price) - min(predicted_price))
  ) |>
  left_join(xgb_city, by = "target_city") |>
  mutate(
    predictability = (xgb_r2 - min(xgb_r2)) /
      (max(xgb_r2) - min(xgb_r2))
  )

# Join screening-based norms
if (exists("city_screening")) {
  screening_norm <- city_screening |>
    filter(department_code %in% TARGET_DEPTS) |>
    select(city_name, sunshine_norm, far_right_norm)
  criteria <- criteria |>
    left_join(
      screening_norm,
      by = c("target_city" = "city_name")
    )
} else {
  criteria <- criteria |>
    mutate(sunshine_norm = NA_real_,
           far_right_norm = NA_real_)
}

# Join cluster diversity
if (exists("commune_summary") &&
    "cluster" %in% names(commune_summary)) {
  criteria <- criteria |>
    left_join(
      cluster_diversity |>
        select(target_city, n_clusters),
      by = "target_city"
    ) |>
    mutate(market_diversity = n_clusters / FINAL_K)
} else {
  criteria <- criteria |>
    mutate(n_clusters = NA_integer_,
           market_diversity = NA_real_)
}

# Pivot to long format for faceted Cleveland dot plot
criteria_long <- criteria |>
  select(
    target_city, affordability, sunshine_norm,
    far_right_norm, predictability, market_diversity
  ) |>
  pivot_longer(
    cols = -target_city,
    names_to = "dimension",
    values_to = "score"
  ) |>
  mutate(
    dimension = factor(
      dimension,
      levels = c(
        "affordability", "sunshine_norm",
        "far_right_norm", "predictability",
        "market_diversity"
      ),
      labels = c(
        "Affordability", "Sunshine",
        "Political Alignment",
        "Model Predictability", "Market Diversity"
      )
    )
  )

ggplot(
  criteria_long,
  aes(
    x = score,
    y = fct_reorder(target_city, score, .fun = mean),
    color = target_city
  )
) +
  geom_point(size = 3) +
  facet_wrap(~ dimension, ncol = 1, scales = "free_x") +
  scale_color_manual(values = city_colors) +
  scale_x_continuous(
    limits = c(0, 1),
    labels = scales::percent_format()
  ) +
  labs(
    title = "Multi-Criteria City Comparison",
    subtitle = "All dimensions normalized 0\u20131 (higher = better)",
    x = "Score", y = NULL
  ) +
  theme_minimal(base_size = 12) +
  theme(
    legend.position = "none",
    strip.text = element_text(face = "bold"),
    panel.spacing.y = unit(1, "lines")
  )

### 5.3.5 Final Ranking with Blended Score ----

cat("\n--- 5.3.5 Final ranking ---\n")
cat("Composite: 50% lifestyle + 30% affordability",
    "+ 20% predictability\n\n")

# Build ranking table
final_ranking <- criteria |>
  select(target_city, affordability, predictability)

if (exists("city_screening")) {
  screening_scores <- city_screening |>
    filter(department_code %in% TARGET_DEPTS) |>
    select(city_name, composite_score)
  final_ranking <- final_ranking |>
    left_join(
      screening_scores,
      by = c("target_city" = "city_name")
    )
} else {
  final_ranking <- final_ranking |>
    mutate(composite_score = NA_real_)
}

final_ranking <- final_ranking |>
  mutate(
    blended_score = 0.50 * composite_score +
      0.30 * affordability +
      0.20 * predictability
  ) |>
  arrange(desc(blended_score)) |>
  mutate(rank = row_number())

final_ranking |>
  mutate(
    across(
      c(composite_score, affordability,
        predictability, blended_score),
      ~ sprintf("%.3f", .)
    )
  ) |>
  select(
    rank, target_city, blended_score,
    composite_score, affordability, predictability
  ) |>
  print()

top_city   <- final_ranking$target_city[1]
runner_up  <- final_ranking$target_city[2]

cat("\nRecommendation:", top_city, "(blended:",
    sprintf("%.3f", final_ranking$blended_score[1]), ") |",
    "Runner-up:", runner_up, "(blended:",
    sprintf("%.3f", final_ranking$blended_score[2]), ")\n")

### 5.3.6 City Profiles ----

cat("\n--- 5.3.6 City profiles ---\n\n")

for (i in seq_len(nrow(final_ranking))) {
  city <- final_ranking$target_city[i]
  rnk  <- final_ranking$rank[i]

  # Retrieve city-specific data
  re   <- city_summary |> filter(target_city == city)
  pred <- std_house    |> filter(target_city == city)
  crit <- criteria     |> filter(target_city == city)

  cat(paste0("--- #", rnk, " ", city, " ---\n"))
  cat("  Standard house prediction: \u20ac",
      scales::comma(round(pred$predicted_price)),
      "\n", sep = "")
  cat("  Median price (all transactions): \u20ac",
      scales::comma(round(re$median_price)),
      "\n", sep = "")
  cat("  Transactions: ",
      scales::comma(re$n_transactions),
      "\n", sep = "")

  if (exists("city_screening")) {
    cs <- city_screening |>
      filter(city_name == city,
             department_code %in% TARGET_DEPTS)
    if (nrow(cs) > 0) {
      cat("  Sunshine: ",
          round(cs$sunshine_hours_annual),
          " hours/year\n", sep = "")
      cat("  Far-right vote: ",
          sprintf("%.1f%%", cs$pct_far_right),
          "\n", sep = "")
    }
  }

  cat("  XGBoost R\u00b2: ",
      sprintf("%.3f", crit$xgb_r2),
      "\n", sep = "")

  if ("n_clusters" %in% names(crit) &&
      !is.na(crit$n_clusters)) {
    cat("  Market clusters: ", crit$n_clusters,
        " of ", FINAL_K, "\n", sep = "")
  }

  cat("\n")
}

cat("=== End of City Rankings ===\n")

#-------------------------------------------------------------------------------
# 6. SAVE REPORT DATA
#-------------------------------------------------------------------------------
# Save all objects needed by cyo_report.Rmd so the report can be knit
# without retraining models. Run this script before knitting the report.

report_data <- list(
  dvf_houses = dvf_houses,
  city_screening = city_screening,
  train_nrow = nrow(train),
  test_nrow = nrow(test),
  all_results = all_results,
  fi_combined = fi_combined,
  commune_summary = commune_summary,
  commune_scaled = commune_scaled,
  std_house = std_house,
  final_ranking = final_ranking,
  city_colors = city_colors,
  model_colors = model_colors,
  TARGET_DEPTS = TARGET_DEPTS,
  FINAL_K = FINAL_K
)

saveRDS(report_data, file.path(DATA_PROCESSED, "report_data.RDS"))
cat("\nSaved report data to:", file.path(DATA_PROCESSED, "report_data.RDS"), "\n")

#-------------------------------------------------------------------------------
# END OF SCRIPT
#-------------------------------------------------------------------------------